import gradio as gr
import base64
import time
import random
import asyncio

class ChatModel:
    def __init__(self):
        self.history = [{"role": "system", "content": "Sen TÃ¼rkÃ§e konuÅŸan genel amaÃ§lÄ± bir asistansÄ±n. Her zaman kullanÄ±cÄ±nÄ±n verdiÄŸi talimatlarÄ± doÄŸru, kÄ±sa ve gÃ¼zel bir gramer ile yerine getir."}]

    async def generate_response(self, user_input):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        device = "cuda"
        model_id = "Karayel-DDI/KARAYEL-LLM-q4"

        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
        )

        self.history.append({"role": "user", "content": user_input})

        #inputs = tokenizer.apply_chat_template(
        #    self.history,
        #    add_generation_prompt=True,
        #    return_tensors="pt"
        #).to(model.device)
        
        
        
        input_ids = tokenizer.apply_chat_template(
            self.history,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
        
        terminators = [
            tokenizer.eos_token_id,
            tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        outputs = model.generate(
            input_ids,
            #max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )

        #outputs = model.generate(
        #    **inputs,
        #    use_cache=True,
        #    do_sample=True,
        #    top_k=50,
        #    top_p=0.60,
        #    temperature=0.3,
        #    repetition_penalty=1.1
        #)

        out_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
        out_text = out_text.split('assistant\n\n')[-1]
        self.history.append({"role": "assistant", "content": out_text})

        return out_text

    def clear_history(self):
        self.history = [{"role": "system", "content": "Sen TÃ¼rkÃ§e konuÅŸan genel amaÃ§lÄ± bir asistansÄ±n. Her zaman kullanÄ±cÄ±nÄ±n verdiÄŸi talimatlarÄ± doÄŸru, kÄ±sa ve gÃ¼zel bir gramer ile yerine getir."}]

chat_model = ChatModel()

with open("logo.jpeg", "rb") as image_file:
    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')

css = """
<style>
    .logo-container {
        position: absolute;
        top: 10px;
        left: 10px;
        z-index: 100;
    }
    .logo {
        max-width: 100px;
        max-height: 80px;
    }
    .gradio-container {
        padding-top: 60px;
    }
    .thinking {
        font-style: italic;
        color: gray;
    }
</style>
"""

thinking_messages = [
    "ğŸ§  DÃ¼ÅŸÃ¼nÃ¼yorum...",
    "ğŸ’¡ Fikir Ã¼retiyorum...",
    "ğŸ” AraÅŸtÄ±rÄ±yorum...",
    "ğŸ“š Bilgilerimi gÃ¶zden geÃ§iriyorum...",
    "âš™ï¸ Ä°ÅŸleniyor...",
    "ğŸ¤” HÄ±mm, bir dÃ¼ÅŸÃ¼neyim...",
    "ğŸŒŸ Ä°lham peÅŸindeyim...",
    "ğŸ§© ParÃ§alarÄ± birleÅŸtiriyorum...",
    "ğŸ”® GeleceÄŸi okuyorum...",
    "ğŸ­ CevabÄ± formÃ¼le ediyorum..."
]

with gr.Blocks(css=css) as demo:
    gr.HTML(f'''
    <div class="logo-container">
        <img src="data:image/png;base64,{encoded_string}" alt="Logo" class="logo">
    </div>
    ''')
    
    gr.Markdown("# Karayel Rehberlik UygulamasÄ±")
    
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="MesajÄ±nÄ±zÄ± yazÄ±n")
    clear = gr.Button("Temizle", elem_classes=["primary-btn"])

    async def user(user_message, history):
        history = history + [[user_message, None]]
        return "", history

    async def bot(history):
        user_message = history[-1][0]
        thinking_message = random.choice(thinking_messages)
        
        # DÃ¼ÅŸÃ¼nme mesajÄ±nÄ± gÃ¶ster
        history[-1][1] = f"<span class='thinking'>{thinking_message}</span>"
        yield history
        
        # Model cevabÄ±nÄ± bekle
        bot_message = await chat_model.generate_response(user_message)
        
        # CevabÄ± karakter karakter gÃ¶ster
        history[-1][1] = ""
        for character in bot_message:
            history[-1][1] += character
            time.sleep(0.05)
            yield history

    def clear_history():
        chat_model.clear_history()
        return None

    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot, chatbot, chatbot
    )
    clear.click(clear_history, None, chatbot, queue=False)

demo.launch()